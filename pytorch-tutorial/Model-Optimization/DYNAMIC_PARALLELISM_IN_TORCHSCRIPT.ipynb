{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DYNAMIC PARALLELISM IN TORCHSCRIPT.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "HiUj9EErO3A9",
        "outputId": "f039c32d-a40e-414c-c637-b34bc3e98d84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import torch\n",
        "\n",
        "def foo(x):\n",
        "    return torch.neg(x)\n",
        "\n",
        "@torch.jit.script\n",
        "def example(x):\n",
        "    # Call `foo` using parallelism:\n",
        "    # First, we \"fork\" off a task. This task will run `foo` with argument `x`\n",
        "    future = torch.jit.fork(foo, x)\n",
        "\n",
        "    # Call `foo` normally\n",
        "    x_normal = foo(x)\n",
        "\n",
        "    # Second, we \"wait\" on the task. Since the task may be running in\n",
        "    # parallel, we have to \"wait\" for its result to become available.\n",
        "    # Notice that by having lines of code between the \"fork()\" and \"wait()\"\n",
        "    # call for a given Future, we can overlap computations so that they\n",
        "    # run in parallel.\n",
        "    x_parallel = torch.jit.wait(future)\n",
        "\n",
        "    return x_normal, x_parallel\n",
        "\n",
        "print(example(torch.ones(1))) # (-1., -1.)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(tensor([-1.]), tensor([-1.]))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YT-WZEWuPXqd",
        "outputId": "c6a7c413-b4af-4a7d-e971-1a869fe12a41",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import torch\n",
        "from typing import List\n",
        "\n",
        "def foo(x):\n",
        "    return torch.neg(x)\n",
        "\n",
        "@torch.jit.script\n",
        "def example(x):\n",
        "    futures : List[torch.jit.Future[torch.Tensor]] = []\n",
        "    for _ in range(100):\n",
        "        futures.append(torch.jit.fork(foo, x))\n",
        "\n",
        "    results = []\n",
        "    for future in futures:\n",
        "        results.append(torch.jit.wait(future))\n",
        "\n",
        "    return torch.sum(torch.stack(results))\n",
        "\n",
        "print(example(torch.ones([])))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(-100.)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dRNunKXPdfE",
        "outputId": "7203514f-216d-4c58-fbbd-e3a7f50983cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import torch, time\n",
        "\n",
        "# In RNN parlance, the dimensions we care about are:\n",
        "# # of time-steps (T)\n",
        "# Batch size (B)\n",
        "# Hidden size/number of \"channels\" (C)\n",
        "T, B, C = 50, 50, 1024\n",
        "\n",
        "# A module that defines a single \"bidirectional LSTM\". This is simply two\n",
        "# LSTMs applied to the same sequence, but one in reverse\n",
        "class BidirectionalRecurrentLSTM(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.cell_f = torch.nn.LSTM(input_size=C, hidden_size=C)\n",
        "        self.cell_b = torch.nn.LSTM(input_size=C, hidden_size=C)\n",
        "\n",
        "    def forward(self, x : torch.Tensor) -> torch.Tensor:\n",
        "        # Forward layer\n",
        "        output_f, _ = self.cell_f(x)\n",
        "\n",
        "        # Backward layer. Flip input in the time dimension (dim 0), apply the\n",
        "        # layer, then flip the outputs in the time dimension\n",
        "        x_rev = torch.flip(x, dims=[0])\n",
        "        output_b, _ = self.cell_b(torch.flip(x, dims=[0]))\n",
        "        output_b_rev = torch.flip(output_b, dims=[0])\n",
        "\n",
        "        return torch.cat((output_f, output_b_rev), dim=2)\n",
        "\n",
        "\n",
        "# An \"ensemble\" of `BidirectionalRecurrentLSTM` modules. The modules in the\n",
        "# ensemble are run one-by-one on the same input then their results are\n",
        "# stacked and summed together, returning the combined result.\n",
        "class LSTMEnsemble(torch.nn.Module):\n",
        "    def __init__(self, n_models):\n",
        "        super().__init__()\n",
        "        self.n_models = n_models\n",
        "        self.models = torch.nn.ModuleList([\n",
        "            BidirectionalRecurrentLSTM() for _ in range(self.n_models)])\n",
        "\n",
        "    def forward(self, x : torch.Tensor) -> torch.Tensor:\n",
        "        results = []\n",
        "        for model in self.models:\n",
        "            results.append(model(x))\n",
        "        return torch.stack(results).sum(dim=0)\n",
        "\n",
        "# For a head-to-head comparison to what we're going to do with fork/wait, let's\n",
        "# instantiate the model and compile it with TorchScript\n",
        "ens = torch.jit.script(LSTMEnsemble(n_models=4))\n",
        "\n",
        "# Normally you would pull this input out of an embedding table, but for the\n",
        "# purpose of this demo let's just use random data.\n",
        "x = torch.rand(T, B, C)\n",
        "\n",
        "# Let's run the model once to warm up things like the memory allocator\n",
        "ens(x)\n",
        "\n",
        "x = torch.rand(T, B, C)\n",
        "\n",
        "# Let's see how fast it runs!\n",
        "s = time.time()\n",
        "ens(x)\n",
        "print('Inference took', time.time() - s, ' seconds')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Inference took 6.181236267089844  seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RurTZgllPg-n"
      },
      "source": [
        "def forward(self, x : torch.Tensor) -> torch.Tensor:\n",
        "    # Forward layer - fork() so this can run in parallel to the backward\n",
        "    # layer\n",
        "    future_f = torch.jit.fork(self.cell_f, x)\n",
        "\n",
        "    # Backward layer. Flip input in the time dimension (dim 0), apply the\n",
        "    # layer, then flip the outputs in the time dimension\n",
        "    x_rev = torch.flip(x, dims=[0])\n",
        "    output_b, _ = self.cell_b(torch.flip(x, dims=[0]))\n",
        "    output_b_rev = torch.flip(output_b, dims=[0])\n",
        "\n",
        "    # Retrieve the output from the forward layer. Note this needs to happen\n",
        "    # *after* the stuff we want to parallelize with\n",
        "    output_f, _ = torch.jit.wait(future_f)\n",
        "\n",
        "    return torch.cat((output_f, output_b_rev), dim=2)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RNJU1vWXPwsX"
      },
      "source": [
        "def forward(self, x : torch.Tensor) -> torch.Tensor:\n",
        "    # Launch tasks for each model\n",
        "    futures : List[torch.jit.Future[torch.Tensor]] = []\n",
        "    for model in self.models:\n",
        "        futures.append(torch.jit.fork(model, x))\n",
        "\n",
        "    # Collect the results from the launched tasks\n",
        "    results : List[torch.Tensor] = []\n",
        "    for future in futures:\n",
        "        results.append(torch.jit.wait(future))\n",
        "\n",
        "    return torch.stack(results).sum(dim=0)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "if-sh4vRP8Fn"
      },
      "source": [
        "def forward(self, x : torch.Tensor) -> torch.Tensor:\n",
        "    futures = [torch.jit.fork(model, x) for model in self.models]\n",
        "    results = [torch.jit.wait(fut) for fut in futures]\n",
        "    return torch.stack(results).sum(dim=0)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4xB2qKuQKxs"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}