{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "neural_networks_tutorial.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQlgvBtplGJ7"
      },
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-Xy0gl6lGJ_"
      },
      "source": [
        "\n",
        "Neural Networks\n",
        "===============\n",
        "\n",
        "Neural networks can be constructed using the ``torch.nn`` package.\n",
        "\n",
        "Now that you had a glimpse of ``autograd``, ``nn`` depends on\n",
        "``autograd`` to define models and differentiate them.\n",
        "An ``nn.Module`` contains layers, and a method ``forward(input)``\\ that\n",
        "returns the ``output``.\n",
        "\n",
        "For example, look at this network that classifies digit images:\n",
        "\n",
        ".. figure:: /_static/img/mnist.png\n",
        "   :alt: convnet\n",
        "\n",
        "   convnet\n",
        "\n",
        "It is a simple feed-forward network. It takes the input, feeds it\n",
        "through several layers one after the other, and then finally gives the\n",
        "output.\n",
        "\n",
        "A typical training procedure for a neural network is as follows:\n",
        "\n",
        "- Define the neural network that has some learnable parameters (or\n",
        "  weights)\n",
        "- Iterate over a dataset of inputs\n",
        "- Process input through the network\n",
        "- Compute the loss (how far is the output from being correct)\n",
        "- Propagate gradients back into the network’s parameters\n",
        "- Update the weights of the network, typically using a simple update rule:\n",
        "  ``weight = weight - learning_rate * gradient``\n",
        "\n",
        "Define the network\n",
        "------------------\n",
        "\n",
        "Let’s define this network:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0wBc-N3slGJ_",
        "outputId": "10343529-fbdc-4be4-8971-6777c7632a73",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        # 1 input image channel, 6 output channels, 3x3 square convolution\n",
        "        # kernel\n",
        "        self.conv1 = nn.Conv2d(1, 6, 3)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 3)\n",
        "        # an affine operation: y = Wx + b\n",
        "        self.fc1 = nn.Linear(16 * 6 * 6, 120)  # 6*6 from image dimension \n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Max pooling over a (2, 2) window\n",
        "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
        "        # If the size is a square you can only specify a single number\n",
        "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
        "        x = x.view(-1, self.num_flat_features(x))\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "    def num_flat_features(self, x):\n",
        "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
        "        num_features = 1\n",
        "        for s in size:\n",
        "            num_features *= s\n",
        "        return num_features\n",
        "\n",
        "\n",
        "net = Net()\n",
        "print(net)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Net(\n",
            "  (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (fc1): Linear(in_features=576, out_features=120, bias=True)\n",
            "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
            "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETiXJBNhlGKC"
      },
      "source": [
        "You just have to define the ``forward`` function, and the ``backward``\n",
        "function (where gradients are computed) is automatically defined for you\n",
        "using ``autograd``.\n",
        "You can use any of the Tensor operations in the ``forward`` function.\n",
        "\n",
        "The learnable parameters of a model are returned by ``net.parameters()``\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5patbG3lGKC",
        "outputId": "802344ea-4234-4422-d558-dfdb5765b71d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "params = list(net.parameters())\n",
        "print(params)\n",
        "print(len(params))\n",
        "print(params[0].size())  # conv1's .weight"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Parameter containing:\n",
            "tensor([[[[-0.0728,  0.0137, -0.2647],\n",
            "          [ 0.2827,  0.0364, -0.0027],\n",
            "          [-0.2196, -0.2022, -0.0274]]],\n",
            "\n",
            "\n",
            "        [[[-0.2715,  0.1602,  0.0968],\n",
            "          [ 0.2414,  0.1640,  0.3151],\n",
            "          [-0.2039, -0.1826,  0.1748]]],\n",
            "\n",
            "\n",
            "        [[[ 0.2098, -0.2717,  0.1768],\n",
            "          [-0.2057,  0.2271, -0.0810],\n",
            "          [ 0.0131,  0.1462, -0.0401]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0335,  0.2314, -0.3141],\n",
            "          [ 0.0183, -0.2629, -0.0559],\n",
            "          [-0.0581, -0.3130,  0.2151]]],\n",
            "\n",
            "\n",
            "        [[[ 0.1866, -0.2249, -0.3067],\n",
            "          [-0.1062, -0.3058, -0.3292],\n",
            "          [ 0.0525, -0.1367, -0.2953]]],\n",
            "\n",
            "\n",
            "        [[[ 0.2637, -0.2623, -0.0900],\n",
            "          [-0.0076,  0.1277,  0.2777],\n",
            "          [ 0.3007,  0.2993, -0.1378]]]], requires_grad=True), Parameter containing:\n",
            "tensor([ 0.0550, -0.0569,  0.2199, -0.3009,  0.2317, -0.1846],\n",
            "       requires_grad=True), Parameter containing:\n",
            "tensor([[[[ 0.1263,  0.0277,  0.0700],\n",
            "          [-0.0473,  0.1051, -0.0037],\n",
            "          [-0.0901,  0.0492,  0.0962]],\n",
            "\n",
            "         [[-0.0242,  0.0565, -0.0652],\n",
            "          [-0.0842,  0.0602, -0.1161],\n",
            "          [ 0.0558,  0.0310,  0.0599]],\n",
            "\n",
            "         [[-0.1031, -0.0173,  0.0133],\n",
            "          [ 0.1101, -0.0730,  0.1040],\n",
            "          [ 0.0330, -0.0745,  0.0026]],\n",
            "\n",
            "         [[ 0.1160,  0.0293, -0.1073],\n",
            "          [ 0.0172, -0.0301,  0.1031],\n",
            "          [-0.0446, -0.0535,  0.0965]],\n",
            "\n",
            "         [[ 0.1166, -0.0950,  0.1337],\n",
            "          [-0.1125, -0.1194,  0.1093],\n",
            "          [-0.0319, -0.0305,  0.0599]],\n",
            "\n",
            "         [[ 0.1268, -0.0147,  0.0105],\n",
            "          [ 0.0297, -0.0129, -0.0304],\n",
            "          [-0.0432, -0.1348,  0.0726]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0071,  0.0390, -0.0992],\n",
            "          [-0.1066,  0.1110,  0.1248],\n",
            "          [-0.1119,  0.0225, -0.0177]],\n",
            "\n",
            "         [[ 0.1269, -0.0886,  0.0594],\n",
            "          [ 0.0618, -0.0424, -0.0392],\n",
            "          [-0.0996,  0.0517, -0.0304]],\n",
            "\n",
            "         [[-0.0270, -0.0543,  0.1294],\n",
            "          [ 0.0269, -0.0188,  0.0345],\n",
            "          [-0.1349, -0.1047, -0.0311]],\n",
            "\n",
            "         [[-0.0834, -0.1099,  0.0787],\n",
            "          [-0.0046, -0.1009,  0.0573],\n",
            "          [ 0.0084,  0.0081,  0.0829]],\n",
            "\n",
            "         [[ 0.0881,  0.0413, -0.0104],\n",
            "          [-0.1292,  0.0607, -0.1232],\n",
            "          [-0.0819,  0.0064,  0.0368]],\n",
            "\n",
            "         [[ 0.0321,  0.0184,  0.0897],\n",
            "          [-0.0698, -0.0973,  0.0374],\n",
            "          [-0.1141,  0.0651,  0.0855]]],\n",
            "\n",
            "\n",
            "        [[[ 0.1145, -0.1105,  0.0086],\n",
            "          [ 0.1063,  0.1211,  0.1163],\n",
            "          [ 0.0026, -0.0044, -0.0930]],\n",
            "\n",
            "         [[-0.1211, -0.0730, -0.0004],\n",
            "          [ 0.1263,  0.0510, -0.1165],\n",
            "          [-0.1344,  0.0604,  0.1068]],\n",
            "\n",
            "         [[ 0.0599,  0.0783,  0.0271],\n",
            "          [ 0.1091, -0.1101, -0.1228],\n",
            "          [-0.0468, -0.0285, -0.0747]],\n",
            "\n",
            "         [[-0.0527,  0.1106, -0.0260],\n",
            "          [ 0.1001,  0.0422, -0.0775],\n",
            "          [-0.1238, -0.0391,  0.0753]],\n",
            "\n",
            "         [[ 0.1193, -0.1033, -0.0276],\n",
            "          [-0.0977, -0.0051,  0.0786],\n",
            "          [-0.1083,  0.1101,  0.1235]],\n",
            "\n",
            "         [[-0.1137,  0.0107,  0.0473],\n",
            "          [-0.0456, -0.1190,  0.0198],\n",
            "          [-0.0559, -0.1038, -0.0543]]],\n",
            "\n",
            "\n",
            "        [[[-0.0970,  0.1294, -0.0412],\n",
            "          [ 0.0790,  0.1282,  0.0802],\n",
            "          [ 0.1098,  0.0394,  0.1298]],\n",
            "\n",
            "         [[-0.0451,  0.0819, -0.0013],\n",
            "          [ 0.1056, -0.0669,  0.0895],\n",
            "          [-0.1141,  0.0324, -0.1054]],\n",
            "\n",
            "         [[ 0.1286,  0.1297, -0.1216],\n",
            "          [ 0.0475,  0.1241, -0.1102],\n",
            "          [-0.0022, -0.0675,  0.0890]],\n",
            "\n",
            "         [[ 0.0533, -0.0332, -0.1313],\n",
            "          [ 0.0814, -0.0698, -0.1315],\n",
            "          [-0.1341,  0.0461,  0.0168]],\n",
            "\n",
            "         [[ 0.0704, -0.0005,  0.0204],\n",
            "          [ 0.0115, -0.0306,  0.0855],\n",
            "          [ 0.0718, -0.0082,  0.1297]],\n",
            "\n",
            "         [[-0.0202,  0.0386,  0.0913],\n",
            "          [-0.0091, -0.1357,  0.0817],\n",
            "          [-0.0691, -0.1192, -0.1223]]],\n",
            "\n",
            "\n",
            "        [[[-0.0350,  0.0219,  0.1310],\n",
            "          [-0.1336,  0.1319, -0.0718],\n",
            "          [-0.0704, -0.0991,  0.0741]],\n",
            "\n",
            "         [[-0.0077, -0.0308, -0.1227],\n",
            "          [ 0.0402, -0.1065,  0.0857],\n",
            "          [ 0.1131, -0.0764, -0.0344]],\n",
            "\n",
            "         [[ 0.0836,  0.0306, -0.1131],\n",
            "          [-0.1042,  0.0255,  0.0432],\n",
            "          [ 0.1107, -0.0110,  0.0669]],\n",
            "\n",
            "         [[-0.0225,  0.0797,  0.1226],\n",
            "          [-0.1350,  0.1205,  0.1117],\n",
            "          [-0.0499, -0.0015, -0.0433]],\n",
            "\n",
            "         [[ 0.0884,  0.0881,  0.0041],\n",
            "          [-0.0433, -0.0659,  0.0800],\n",
            "          [-0.0756, -0.0789, -0.1162]],\n",
            "\n",
            "         [[ 0.0295, -0.1005, -0.0767],\n",
            "          [-0.1057, -0.0858, -0.0354],\n",
            "          [ 0.1220,  0.0709, -0.1013]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0313, -0.1171, -0.0721],\n",
            "          [ 0.0357,  0.0826, -0.0663],\n",
            "          [-0.0828, -0.1163, -0.0813]],\n",
            "\n",
            "         [[-0.1081, -0.0241,  0.0655],\n",
            "          [ 0.0211, -0.0308,  0.0904],\n",
            "          [ 0.0592,  0.1333, -0.0394]],\n",
            "\n",
            "         [[-0.0876, -0.0684,  0.0925],\n",
            "          [-0.1319,  0.0976,  0.0392],\n",
            "          [-0.0438,  0.0167,  0.0239]],\n",
            "\n",
            "         [[-0.0907,  0.0389,  0.1056],\n",
            "          [ 0.0851, -0.1249,  0.0498],\n",
            "          [ 0.1146,  0.1256, -0.0016]],\n",
            "\n",
            "         [[-0.1048, -0.0915, -0.0041],\n",
            "          [-0.0700,  0.0691,  0.1158],\n",
            "          [-0.1289, -0.0348,  0.0549]],\n",
            "\n",
            "         [[-0.1236,  0.0841,  0.0867],\n",
            "          [ 0.0812, -0.1155,  0.0821],\n",
            "          [-0.0111,  0.1133, -0.1123]]],\n",
            "\n",
            "\n",
            "        [[[ 0.1061, -0.0011,  0.0681],\n",
            "          [-0.1236, -0.0912,  0.0620],\n",
            "          [-0.0075, -0.0818, -0.0721]],\n",
            "\n",
            "         [[-0.0721,  0.0265, -0.0240],\n",
            "          [-0.0784,  0.0650, -0.0504],\n",
            "          [ 0.0115,  0.0445, -0.0169]],\n",
            "\n",
            "         [[-0.1346, -0.1160,  0.0352],\n",
            "          [ 0.1215, -0.1141,  0.0576],\n",
            "          [ 0.0805, -0.0266,  0.0232]],\n",
            "\n",
            "         [[ 0.0103,  0.0784, -0.0143],\n",
            "          [ 0.0531,  0.0828,  0.0642],\n",
            "          [ 0.0356,  0.1183,  0.0999]],\n",
            "\n",
            "         [[ 0.1059, -0.1141, -0.0647],\n",
            "          [ 0.1078,  0.1190, -0.1067],\n",
            "          [ 0.0930,  0.1229,  0.0109]],\n",
            "\n",
            "         [[ 0.0859,  0.0737, -0.0700],\n",
            "          [ 0.0626, -0.0575,  0.1164],\n",
            "          [-0.0854, -0.0165,  0.0312]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0188, -0.0848,  0.0467],\n",
            "          [-0.0892, -0.0438,  0.1048],\n",
            "          [-0.1136,  0.0827, -0.1079]],\n",
            "\n",
            "         [[-0.1188,  0.0743,  0.0290],\n",
            "          [ 0.0909,  0.0329, -0.0353],\n",
            "          [ 0.0867, -0.0784, -0.1184]],\n",
            "\n",
            "         [[ 0.0975,  0.1171,  0.0214],\n",
            "          [ 0.0438,  0.0363, -0.0798],\n",
            "          [-0.1343, -0.0490,  0.1213]],\n",
            "\n",
            "         [[ 0.0388,  0.0842, -0.0795],\n",
            "          [-0.0328,  0.0525,  0.1220],\n",
            "          [-0.1238, -0.0501, -0.1175]],\n",
            "\n",
            "         [[ 0.0641,  0.0866, -0.0258],\n",
            "          [ 0.0035,  0.1278,  0.0822],\n",
            "          [-0.0469,  0.0717, -0.1279]],\n",
            "\n",
            "         [[-0.0648,  0.0874,  0.0778],\n",
            "          [ 0.1224,  0.0581, -0.0749],\n",
            "          [ 0.0616, -0.0308, -0.0865]]],\n",
            "\n",
            "\n",
            "        [[[-0.0790, -0.0066, -0.1295],\n",
            "          [ 0.0473, -0.0404,  0.0199],\n",
            "          [ 0.0858, -0.0351,  0.1307]],\n",
            "\n",
            "         [[ 0.1195, -0.1109, -0.0556],\n",
            "          [-0.1226,  0.0807,  0.0917],\n",
            "          [-0.0887,  0.0471, -0.1335]],\n",
            "\n",
            "         [[-0.0844,  0.0033, -0.0065],\n",
            "          [-0.0657,  0.0439, -0.0995],\n",
            "          [ 0.1043,  0.0649, -0.0898]],\n",
            "\n",
            "         [[ 0.0693,  0.0613, -0.0930],\n",
            "          [ 0.1109, -0.0647, -0.0083],\n",
            "          [ 0.0901, -0.1015,  0.0714]],\n",
            "\n",
            "         [[-0.1245,  0.0142, -0.0925],\n",
            "          [-0.0760,  0.0863,  0.0240],\n",
            "          [ 0.0577,  0.0181, -0.1143]],\n",
            "\n",
            "         [[ 0.1214,  0.0091,  0.0265],\n",
            "          [ 0.1322, -0.1064,  0.0470],\n",
            "          [ 0.1028,  0.0149, -0.0712]]],\n",
            "\n",
            "\n",
            "        [[[ 0.1295,  0.0377, -0.0318],\n",
            "          [-0.1053,  0.0141,  0.0742],\n",
            "          [-0.0007, -0.0488, -0.0112]],\n",
            "\n",
            "         [[ 0.0258, -0.0947, -0.1208],\n",
            "          [-0.0571,  0.0331,  0.0468],\n",
            "          [-0.0107,  0.0291,  0.0613]],\n",
            "\n",
            "         [[-0.1037, -0.0807, -0.0272],\n",
            "          [-0.1034, -0.0974, -0.0026],\n",
            "          [-0.0506,  0.0638, -0.0187]],\n",
            "\n",
            "         [[ 0.1057, -0.0443, -0.1299],\n",
            "          [-0.0475,  0.0641, -0.0367],\n",
            "          [-0.0826,  0.0274, -0.1333]],\n",
            "\n",
            "         [[-0.0648, -0.0509,  0.0527],\n",
            "          [ 0.0620, -0.0374, -0.0581],\n",
            "          [ 0.1326, -0.0161,  0.0448]],\n",
            "\n",
            "         [[-0.0365, -0.0033,  0.0581],\n",
            "          [-0.0489, -0.0558,  0.1087],\n",
            "          [ 0.0046, -0.0123, -0.0790]]],\n",
            "\n",
            "\n",
            "        [[[ 0.1238, -0.0073,  0.0498],\n",
            "          [-0.0169, -0.0519,  0.1308],\n",
            "          [ 0.1288, -0.1210, -0.1201]],\n",
            "\n",
            "         [[ 0.0197,  0.0842, -0.0295],\n",
            "          [-0.1021,  0.0632,  0.0537],\n",
            "          [-0.0324, -0.1178, -0.0458]],\n",
            "\n",
            "         [[-0.1303,  0.0225,  0.1282],\n",
            "          [-0.1071,  0.0882,  0.0922],\n",
            "          [ 0.0574,  0.0475, -0.0223]],\n",
            "\n",
            "         [[ 0.0969, -0.0382, -0.0324],\n",
            "          [-0.1286, -0.1024, -0.0763],\n",
            "          [ 0.0338, -0.1052,  0.0618]],\n",
            "\n",
            "         [[ 0.0346, -0.0352, -0.0755],\n",
            "          [-0.0649,  0.0742,  0.0921],\n",
            "          [-0.0345,  0.0725, -0.0841]],\n",
            "\n",
            "         [[-0.0435, -0.1178, -0.0522],\n",
            "          [ 0.0314, -0.0430,  0.1198],\n",
            "          [-0.0699, -0.0388, -0.0457]]],\n",
            "\n",
            "\n",
            "        [[[ 0.1066,  0.0243, -0.1014],\n",
            "          [ 0.0993, -0.0644,  0.0867],\n",
            "          [-0.1258, -0.0508, -0.0995]],\n",
            "\n",
            "         [[-0.1333, -0.0964, -0.0113],\n",
            "          [-0.0027, -0.0250,  0.0220],\n",
            "          [-0.1101,  0.1214,  0.0992]],\n",
            "\n",
            "         [[ 0.0227, -0.0989,  0.0609],\n",
            "          [-0.0841,  0.0673,  0.1009],\n",
            "          [-0.1240,  0.1094, -0.1273]],\n",
            "\n",
            "         [[-0.0627,  0.0914,  0.0833],\n",
            "          [-0.1216, -0.0021,  0.0789],\n",
            "          [-0.0716,  0.1105, -0.0196]],\n",
            "\n",
            "         [[-0.0273, -0.0663, -0.1156],\n",
            "          [ 0.0498,  0.0899, -0.0977],\n",
            "          [-0.0798, -0.0965,  0.1219]],\n",
            "\n",
            "         [[ 0.1003,  0.0625,  0.1112],\n",
            "          [ 0.0844, -0.0744, -0.0292],\n",
            "          [ 0.0471, -0.0327, -0.0534]]],\n",
            "\n",
            "\n",
            "        [[[-0.0634, -0.0398, -0.1015],\n",
            "          [ 0.0486, -0.0883, -0.1072],\n",
            "          [-0.0366, -0.0671,  0.0781]],\n",
            "\n",
            "         [[ 0.1049, -0.0529,  0.0016],\n",
            "          [ 0.1300,  0.0248, -0.0340],\n",
            "          [-0.1321, -0.0290,  0.0710]],\n",
            "\n",
            "         [[ 0.0644,  0.1248,  0.0987],\n",
            "          [-0.0723,  0.1296, -0.1324],\n",
            "          [ 0.0670,  0.0426, -0.1121]],\n",
            "\n",
            "         [[ 0.0252,  0.0168, -0.0275],\n",
            "          [ 0.0579, -0.0709,  0.1286],\n",
            "          [ 0.0120, -0.1007, -0.0168]],\n",
            "\n",
            "         [[ 0.0633,  0.0832,  0.0108],\n",
            "          [-0.0118, -0.0720,  0.1305],\n",
            "          [-0.0225, -0.0885, -0.0522]],\n",
            "\n",
            "         [[ 0.1224, -0.1119,  0.1320],\n",
            "          [ 0.1154,  0.0809,  0.0732],\n",
            "          [-0.0546, -0.0820, -0.0726]]],\n",
            "\n",
            "\n",
            "        [[[-0.0750,  0.0768, -0.0834],\n",
            "          [ 0.0598, -0.0764, -0.1183],\n",
            "          [ 0.1055, -0.1080,  0.1002]],\n",
            "\n",
            "         [[-0.0622,  0.1334, -0.1142],\n",
            "          [ 0.0988, -0.0881, -0.0343],\n",
            "          [ 0.0617, -0.0584,  0.1215]],\n",
            "\n",
            "         [[-0.1062, -0.1271,  0.0663],\n",
            "          [ 0.0203,  0.0804, -0.0538],\n",
            "          [ 0.0475,  0.0124,  0.0786]],\n",
            "\n",
            "         [[ 0.0464,  0.0761,  0.0593],\n",
            "          [-0.0382,  0.0881,  0.1011],\n",
            "          [ 0.1350, -0.1339,  0.0305]],\n",
            "\n",
            "         [[-0.0913, -0.1020, -0.0925],\n",
            "          [-0.0872, -0.0874, -0.0415],\n",
            "          [ 0.0700,  0.0783,  0.0775]],\n",
            "\n",
            "         [[ 0.0734,  0.0918, -0.0397],\n",
            "          [-0.0927, -0.0845,  0.1116],\n",
            "          [-0.1210,  0.0696,  0.1318]]],\n",
            "\n",
            "\n",
            "        [[[-0.0213, -0.0179, -0.0308],\n",
            "          [ 0.0043, -0.1119,  0.0166],\n",
            "          [ 0.1086, -0.0295,  0.1304]],\n",
            "\n",
            "         [[-0.1338,  0.1164,  0.0173],\n",
            "          [ 0.0309,  0.0432,  0.0150],\n",
            "          [ 0.0318,  0.0564, -0.1049]],\n",
            "\n",
            "         [[-0.0506, -0.0207, -0.1023],\n",
            "          [-0.1208,  0.0892, -0.0977],\n",
            "          [-0.1206, -0.1037, -0.0905]],\n",
            "\n",
            "         [[-0.0170,  0.0938,  0.0835],\n",
            "          [ 0.0353, -0.0421, -0.0061],\n",
            "          [-0.1067,  0.0900, -0.0387]],\n",
            "\n",
            "         [[-0.1349, -0.0499, -0.0873],\n",
            "          [-0.0147, -0.0788, -0.0613],\n",
            "          [ 0.0163, -0.0520,  0.0277]],\n",
            "\n",
            "         [[-0.1316, -0.0640,  0.0577],\n",
            "          [-0.1212, -0.1188,  0.0179],\n",
            "          [-0.0991, -0.0060, -0.0225]]],\n",
            "\n",
            "\n",
            "        [[[-0.1177,  0.0858, -0.0629],\n",
            "          [-0.0171,  0.0805, -0.0341],\n",
            "          [ 0.1240,  0.0190,  0.0766]],\n",
            "\n",
            "         [[ 0.0715,  0.1185, -0.0187],\n",
            "          [-0.0702,  0.0466, -0.0639],\n",
            "          [-0.0243,  0.0661,  0.0786]],\n",
            "\n",
            "         [[ 0.0428,  0.0947,  0.0846],\n",
            "          [ 0.1220, -0.0437,  0.0055],\n",
            "          [-0.0228, -0.0043, -0.0930]],\n",
            "\n",
            "         [[-0.0805, -0.1102, -0.0216],\n",
            "          [-0.0076,  0.0661,  0.0026],\n",
            "          [ 0.0227,  0.0248,  0.0667]],\n",
            "\n",
            "         [[ 0.0425,  0.0938,  0.0736],\n",
            "          [ 0.0564,  0.1243,  0.0592],\n",
            "          [ 0.1299, -0.0360,  0.0936]],\n",
            "\n",
            "         [[ 0.1028, -0.0214,  0.0215],\n",
            "          [-0.0367, -0.1232, -0.0370],\n",
            "          [-0.0926,  0.0982,  0.0190]]]], requires_grad=True), Parameter containing:\n",
            "tensor([-0.0680, -0.0080, -0.1324, -0.0396, -0.0195,  0.1293,  0.1341,  0.0875,\n",
            "        -0.0475,  0.0435,  0.0651,  0.0133, -0.1104, -0.0334,  0.1144, -0.0923],\n",
            "       requires_grad=True), Parameter containing:\n",
            "tensor([[-0.0194,  0.0370, -0.0102,  ...,  0.0164, -0.0229,  0.0075],\n",
            "        [-0.0212, -0.0064, -0.0061,  ..., -0.0406, -0.0154,  0.0256],\n",
            "        [ 0.0104,  0.0367, -0.0044,  ...,  0.0125, -0.0086,  0.0139],\n",
            "        ...,\n",
            "        [-0.0232,  0.0237, -0.0150,  ..., -0.0264,  0.0243,  0.0105],\n",
            "        [-0.0342, -0.0320,  0.0214,  ..., -0.0295, -0.0409, -0.0161],\n",
            "        [ 0.0322, -0.0304,  0.0325,  ...,  0.0386, -0.0156, -0.0161]],\n",
            "       requires_grad=True), Parameter containing:\n",
            "tensor([ 0.0281, -0.0104,  0.0308,  0.0237,  0.0124,  0.0240, -0.0383,  0.0289,\n",
            "        -0.0304, -0.0121, -0.0189, -0.0119, -0.0026, -0.0388,  0.0102, -0.0139,\n",
            "        -0.0241, -0.0031,  0.0210, -0.0212,  0.0396, -0.0051, -0.0349, -0.0037,\n",
            "         0.0142, -0.0261,  0.0235, -0.0271, -0.0115,  0.0376,  0.0179, -0.0194,\n",
            "        -0.0193,  0.0336, -0.0308,  0.0185,  0.0125,  0.0060,  0.0267,  0.0253,\n",
            "        -0.0127, -0.0245,  0.0319, -0.0347,  0.0227, -0.0266,  0.0366, -0.0162,\n",
            "        -0.0024,  0.0212, -0.0127, -0.0270,  0.0253, -0.0158,  0.0098, -0.0347,\n",
            "         0.0358,  0.0307, -0.0290, -0.0194,  0.0007,  0.0337, -0.0166, -0.0012,\n",
            "         0.0081,  0.0332,  0.0412,  0.0322,  0.0054, -0.0061,  0.0286,  0.0409,\n",
            "         0.0365,  0.0297,  0.0321, -0.0159,  0.0227, -0.0177,  0.0031,  0.0238,\n",
            "        -0.0161, -0.0333, -0.0284,  0.0021,  0.0031, -0.0307, -0.0234,  0.0154,\n",
            "         0.0166,  0.0281, -0.0117, -0.0278,  0.0227, -0.0288,  0.0023,  0.0303,\n",
            "         0.0383, -0.0225,  0.0007,  0.0145,  0.0182, -0.0275,  0.0382, -0.0065,\n",
            "         0.0341,  0.0383, -0.0212, -0.0308,  0.0272,  0.0105, -0.0059,  0.0155,\n",
            "        -0.0225, -0.0307, -0.0403, -0.0126, -0.0213, -0.0034,  0.0416,  0.0374],\n",
            "       requires_grad=True), Parameter containing:\n",
            "tensor([[-0.0456, -0.0625,  0.0431,  ...,  0.0251, -0.0858, -0.0752],\n",
            "        [-0.0750, -0.0751,  0.0652,  ...,  0.0235, -0.0501,  0.0171],\n",
            "        [-0.0333,  0.0269, -0.0450,  ..., -0.0081, -0.0387, -0.0860],\n",
            "        ...,\n",
            "        [ 0.0743, -0.0290,  0.0116,  ...,  0.0120, -0.0239, -0.0136],\n",
            "        [-0.0176, -0.0165, -0.0498,  ..., -0.0140,  0.0682, -0.0243],\n",
            "        [-0.0298, -0.0808,  0.0461,  ...,  0.0379,  0.0863,  0.0185]],\n",
            "       requires_grad=True), Parameter containing:\n",
            "tensor([-0.0212,  0.0480, -0.0677, -0.0427,  0.0314, -0.0411,  0.0254,  0.0002,\n",
            "         0.0715,  0.0841,  0.0020,  0.0134, -0.0066, -0.0818, -0.0007, -0.0564,\n",
            "         0.0911,  0.0266,  0.0197,  0.0312,  0.0766,  0.0460,  0.0615,  0.0886,\n",
            "         0.0467,  0.0820,  0.0566,  0.0853, -0.0311, -0.0517,  0.0045,  0.0119,\n",
            "         0.0413,  0.0322, -0.0156, -0.0318, -0.0002, -0.0210, -0.0774, -0.0090,\n",
            "         0.0714, -0.0211,  0.0741, -0.0079,  0.0243, -0.0346,  0.0172, -0.0563,\n",
            "        -0.0415,  0.0766,  0.0276, -0.0387,  0.0049, -0.0810,  0.0138,  0.0730,\n",
            "        -0.0137, -0.0718,  0.0559,  0.0400, -0.0612,  0.0693,  0.0529,  0.0254,\n",
            "        -0.0743,  0.0730, -0.0461,  0.0204, -0.0029, -0.0818, -0.0559, -0.0143,\n",
            "         0.0012, -0.0377, -0.0201,  0.0517,  0.0491, -0.0322, -0.0866,  0.0235,\n",
            "        -0.0334, -0.0887,  0.0370, -0.0670], requires_grad=True), Parameter containing:\n",
            "tensor([[ 5.4502e-02, -1.0558e-01,  5.8352e-02, -9.8640e-02,  4.0444e-02,\n",
            "         -6.5410e-02,  6.2024e-02, -8.5461e-02,  2.1720e-03,  7.0128e-02,\n",
            "         -4.2730e-02, -9.8842e-02,  1.0688e-01,  6.4283e-02, -1.3154e-02,\n",
            "         -5.5475e-02,  8.4393e-02, -2.0715e-02,  4.0163e-03, -7.8831e-02,\n",
            "          5.7588e-02, -8.7843e-02,  7.8390e-02,  1.0689e-01, -7.1081e-02,\n",
            "         -8.6509e-02,  1.3865e-02,  4.9769e-02, -4.4237e-02, -8.6963e-02,\n",
            "         -6.4603e-02, -5.0876e-02, -8.9354e-02, -2.9025e-03,  7.5605e-02,\n",
            "          4.4206e-02, -6.0909e-02,  2.1117e-02,  3.1219e-02, -9.6384e-03,\n",
            "         -1.8390e-02, -6.2570e-02,  5.4537e-02, -7.4798e-02, -8.7926e-02,\n",
            "         -4.4506e-02,  1.0587e-01, -9.3108e-02,  9.8297e-02,  6.6241e-02,\n",
            "         -9.8336e-02,  9.1246e-02,  3.5225e-02,  6.8044e-02,  4.4575e-02,\n",
            "          1.6976e-02, -4.0673e-03, -5.0370e-02,  5.1386e-02, -6.1677e-02,\n",
            "          1.9146e-03,  9.2699e-02, -1.4280e-02, -1.4435e-03, -2.9761e-02,\n",
            "          7.8339e-02,  7.5179e-02, -9.0466e-02,  9.5936e-02,  1.0680e-01,\n",
            "          3.5490e-02,  1.0231e-01,  2.3556e-02,  7.7160e-03,  8.7389e-02,\n",
            "          1.1608e-02, -5.0499e-02, -3.5491e-02,  5.8458e-02,  1.4846e-02,\n",
            "          6.1641e-02, -6.8580e-02, -1.0546e-01, -8.4846e-02],\n",
            "        [ 6.9278e-02, -7.9759e-02, -1.0461e-01, -3.2359e-02, -3.6675e-02,\n",
            "          5.9207e-02, -7.3865e-02, -6.0092e-02, -8.4693e-02, -5.8810e-02,\n",
            "          8.7556e-02,  5.1603e-02,  2.2313e-02,  9.6013e-02, -6.4866e-02,\n",
            "          2.2035e-02,  4.9866e-02, -2.9734e-02, -2.3944e-02, -7.4053e-02,\n",
            "          1.0729e-01,  4.6085e-03, -5.1728e-02,  7.8836e-02,  9.8164e-02,\n",
            "         -1.0466e-01, -9.0514e-02,  3.9852e-02,  1.0186e-01, -9.7320e-02,\n",
            "         -3.0065e-03, -5.9546e-02,  2.4053e-02,  2.2026e-02, -1.0242e-01,\n",
            "         -3.3233e-02,  5.0608e-03,  1.3545e-03, -5.1288e-02,  9.9221e-02,\n",
            "         -9.1737e-02,  6.7770e-02,  6.0576e-02,  7.4147e-02, -1.6112e-02,\n",
            "         -4.2296e-02, -3.5693e-02, -9.5085e-02, -6.1795e-02,  7.6335e-02,\n",
            "          6.7580e-02,  1.0326e-02, -1.6968e-02, -4.4204e-02,  4.5260e-02,\n",
            "          3.6342e-02,  3.9476e-04, -6.9640e-02,  6.3097e-02,  5.2877e-02,\n",
            "          7.3419e-02, -1.0244e-01, -9.5649e-03, -4.2562e-02, -1.0399e-01,\n",
            "          3.0861e-02, -6.6338e-02, -5.5441e-02, -4.4324e-02,  8.2559e-02,\n",
            "         -3.1392e-02,  5.9773e-02, -4.3381e-02,  9.6572e-02, -4.4507e-02,\n",
            "          8.4617e-03, -1.0812e-01, -3.6118e-02, -5.1702e-02,  9.9070e-02,\n",
            "         -2.3607e-03, -4.3086e-02,  1.2260e-02,  8.4055e-02],\n",
            "        [ 5.0990e-02, -6.6426e-02,  7.6872e-02,  1.0266e-01,  8.2547e-03,\n",
            "         -1.0249e-01, -4.3181e-03,  9.4756e-04,  5.2358e-02, -4.4817e-02,\n",
            "          1.0390e-01, -5.6704e-02,  1.0648e-01, -7.0684e-02, -5.9567e-02,\n",
            "         -8.4323e-02,  6.7968e-02,  3.7355e-02, -7.3914e-02,  7.0736e-02,\n",
            "         -7.0493e-02,  4.8654e-02, -7.7619e-03,  8.8190e-02,  1.5947e-02,\n",
            "          5.1846e-02, -4.1780e-02,  3.2322e-03,  7.3527e-02,  8.8463e-02,\n",
            "          8.3295e-02,  1.6941e-03, -3.2211e-02,  7.6226e-02,  8.0083e-02,\n",
            "         -1.3449e-02,  5.9092e-02, -4.6573e-02, -5.5802e-04,  8.7822e-02,\n",
            "         -3.1246e-02,  7.3519e-03,  9.9712e-02,  9.0600e-02,  1.5664e-02,\n",
            "         -3.8234e-02,  1.2865e-02, -7.0106e-02, -7.0825e-02, -7.8526e-02,\n",
            "          1.0577e-01,  5.5477e-02,  8.8822e-02, -9.4953e-02,  9.4503e-02,\n",
            "         -1.0645e-01,  6.3150e-02, -3.3533e-04,  4.8863e-02, -1.5694e-02,\n",
            "         -4.3312e-02, -8.2112e-02,  8.4730e-02,  6.9368e-02,  9.4816e-02,\n",
            "          1.0237e-01, -9.5351e-03, -2.0982e-02, -7.9193e-02,  9.1544e-02,\n",
            "          4.6133e-02, -1.0333e-01,  8.9709e-02,  6.4738e-02, -1.6440e-02,\n",
            "          5.2289e-02, -1.6444e-02,  5.6048e-02, -1.0356e-01,  7.0229e-02,\n",
            "         -3.7365e-02, -8.6764e-02, -1.2422e-02,  5.9725e-02],\n",
            "        [ 3.8526e-03,  5.9033e-02, -5.3447e-03, -6.7478e-02,  1.0535e-01,\n",
            "          2.9276e-02,  9.9313e-02, -6.1757e-02,  9.8796e-03, -7.7344e-02,\n",
            "          9.2320e-02,  3.6398e-02, -6.2460e-02, -7.9064e-02,  5.2319e-02,\n",
            "          1.0230e-01,  6.6160e-02, -3.3667e-02, -8.6718e-02,  6.7635e-02,\n",
            "          6.7688e-03, -1.0559e-01, -3.9975e-02, -8.6629e-02, -1.0437e-01,\n",
            "         -7.5396e-02,  8.8460e-02,  6.1935e-02,  3.1771e-02,  7.4102e-02,\n",
            "         -5.8419e-03, -5.9234e-02,  3.4468e-02, -9.7890e-02, -1.6898e-02,\n",
            "         -9.2998e-02, -6.1267e-02, -7.8700e-02,  5.5430e-02, -9.0162e-02,\n",
            "          1.5695e-02,  5.8388e-02, -6.0336e-02, -2.8827e-02,  5.2529e-02,\n",
            "          6.1300e-02,  6.4240e-02, -9.5653e-02,  5.2166e-02, -1.0489e-01,\n",
            "         -4.3789e-02, -3.0935e-02, -9.7933e-02,  5.6099e-03, -3.9366e-02,\n",
            "          1.4361e-03,  4.3936e-02,  1.0633e-01,  6.7036e-02, -1.8702e-02,\n",
            "         -1.9538e-02,  8.0127e-02,  2.0481e-02,  1.2270e-02, -1.7970e-02,\n",
            "         -6.5653e-02, -4.3822e-02, -7.2187e-02,  1.3784e-02, -9.6215e-02,\n",
            "         -3.9389e-02, -1.0066e-01,  8.6360e-03,  4.1036e-02,  8.6633e-02,\n",
            "         -6.8622e-03, -9.6223e-02,  7.7265e-02, -5.6195e-02,  7.6621e-02,\n",
            "          4.9672e-02, -5.3355e-02, -8.6384e-02,  1.0354e-01],\n",
            "        [ 7.2463e-03,  4.3617e-02, -8.0690e-02, -4.3693e-02,  3.6905e-02,\n",
            "         -1.9509e-02,  3.5788e-02, -9.1054e-02,  2.9714e-02,  7.3316e-02,\n",
            "          8.6516e-02,  5.9238e-02, -3.5056e-02,  1.5785e-02,  4.0824e-02,\n",
            "          2.2052e-02, -2.5253e-02, -8.0216e-02, -2.4829e-03,  7.8434e-02,\n",
            "          4.6668e-02, -7.1753e-02,  7.7346e-02,  6.7668e-02,  1.2508e-02,\n",
            "          2.7784e-02, -9.7768e-02, -1.3366e-02,  1.9938e-02,  8.7128e-02,\n",
            "         -8.9793e-02, -8.2287e-02,  3.2736e-02, -9.5363e-02,  6.6078e-03,\n",
            "         -4.4605e-02, -3.5884e-03,  4.5295e-02,  6.0370e-02, -1.3338e-02,\n",
            "          6.8269e-02,  2.7683e-02, -7.2493e-02, -7.3833e-02, -6.5317e-03,\n",
            "          6.7125e-02,  7.4958e-02, -7.1102e-02,  8.5452e-02, -1.0531e-01,\n",
            "          6.5622e-02, -9.0827e-02,  9.1736e-02,  5.6570e-02,  4.0473e-02,\n",
            "         -5.1832e-02, -1.5672e-02,  4.2194e-02, -7.0138e-02, -9.4294e-03,\n",
            "         -5.0677e-02, -3.9475e-02,  8.4595e-02, -7.5112e-02, -1.9578e-02,\n",
            "         -1.0336e-01, -1.5902e-02, -1.4124e-02, -7.2453e-03, -7.1758e-02,\n",
            "         -9.6360e-02,  7.0390e-02,  1.0062e-01,  1.0448e-01, -7.7398e-02,\n",
            "         -5.3791e-02,  8.5174e-02, -5.8145e-02, -9.8225e-02, -3.0887e-02,\n",
            "          1.8019e-02,  8.1266e-02, -5.8063e-02, -2.1747e-02],\n",
            "        [-4.1126e-02, -7.9773e-02,  6.3209e-02,  3.0781e-02, -8.9888e-02,\n",
            "         -7.5824e-02,  6.7218e-02, -1.0877e-01, -1.0575e-01, -9.0857e-02,\n",
            "          9.6604e-02,  7.7498e-02,  5.3689e-02, -3.7633e-02,  1.6075e-02,\n",
            "          3.2210e-02, -3.0100e-02, -1.1195e-02,  8.6287e-02,  2.8604e-02,\n",
            "         -1.0711e-01,  5.6765e-02, -3.6003e-02,  9.8916e-02, -2.7157e-02,\n",
            "          8.3099e-02, -8.7808e-03, -8.0477e-02, -4.9120e-02, -7.7338e-02,\n",
            "         -2.4713e-02, -1.3748e-02, -6.2340e-02, -6.1449e-02, -2.2212e-02,\n",
            "         -3.9196e-02,  9.7839e-02, -2.5776e-03, -1.0014e-01,  1.1061e-02,\n",
            "          4.2669e-02, -1.0717e-01, -1.0453e-01, -1.9579e-02, -6.6532e-02,\n",
            "          1.4664e-02, -5.2941e-02, -7.5700e-02, -7.3919e-02,  5.5396e-02,\n",
            "         -1.0144e-01, -6.3218e-02, -7.0513e-02, -2.6267e-02,  5.3849e-02,\n",
            "          8.4616e-03, -6.6107e-02, -9.5439e-02, -7.6078e-02,  4.1388e-02,\n",
            "          2.7507e-02, -1.5429e-02, -7.4398e-02, -2.8117e-02, -4.0773e-02,\n",
            "          7.8192e-02,  8.0046e-02,  9.0290e-02, -9.6675e-02,  5.9740e-02,\n",
            "         -8.5079e-03, -5.5415e-02,  4.8920e-02, -9.1477e-02,  5.6996e-02,\n",
            "         -8.4045e-02,  8.1366e-03, -3.9979e-02,  2.1612e-03,  1.2013e-02,\n",
            "         -3.2388e-02,  4.4327e-02,  4.3052e-02,  3.3105e-02],\n",
            "        [-3.3101e-03, -7.5152e-02,  1.0575e-01,  5.4953e-02,  3.7363e-02,\n",
            "         -7.6342e-02,  6.5905e-03,  4.5055e-02,  8.2129e-02,  9.6488e-02,\n",
            "         -6.2809e-04, -7.9729e-02, -5.9568e-02,  1.5621e-02, -4.0785e-02,\n",
            "          7.1706e-02,  4.7163e-02,  7.6875e-02,  9.7322e-02,  4.0098e-02,\n",
            "          1.0665e-01, -1.0406e-01, -9.0730e-02,  6.7875e-02, -4.2254e-02,\n",
            "         -4.9180e-02, -8.7867e-02,  5.3896e-02,  8.7602e-02, -3.4787e-02,\n",
            "         -2.6668e-04,  1.5175e-02,  4.4460e-02,  2.9922e-02,  2.1459e-02,\n",
            "          7.6713e-03,  5.3646e-02, -4.4812e-02, -1.0855e-01, -7.9717e-04,\n",
            "         -3.1311e-02, -1.7871e-02, -5.4649e-02,  2.9394e-02, -1.2262e-02,\n",
            "         -6.5642e-02,  9.9172e-03, -4.3322e-02,  4.8437e-02,  2.7015e-02,\n",
            "          9.5681e-02,  4.4220e-02,  9.7764e-02,  1.4723e-02, -2.4142e-03,\n",
            "         -5.1202e-02,  8.3961e-02,  9.2449e-02, -6.3106e-02, -4.9579e-03,\n",
            "         -8.9626e-02, -1.4567e-02,  5.3504e-02, -5.8634e-02,  1.0281e-01,\n",
            "         -2.5860e-02,  9.0561e-02, -3.4982e-03,  3.8190e-02, -7.8197e-02,\n",
            "          3.4882e-02, -8.1590e-02,  4.5429e-02,  5.2966e-02, -3.0028e-02,\n",
            "         -6.3150e-02, -3.5668e-02,  2.4366e-02,  8.3807e-02,  9.1635e-02,\n",
            "          4.4100e-02,  8.7187e-02, -1.3476e-02, -3.7043e-02],\n",
            "        [ 7.3930e-02, -8.4044e-02,  3.9813e-02,  4.3747e-02, -7.4456e-02,\n",
            "          1.1042e-02,  9.3094e-03, -9.0035e-03, -8.2927e-02, -7.7571e-02,\n",
            "          6.6883e-02, -8.9274e-02, -5.7391e-02,  7.4869e-02,  4.0385e-02,\n",
            "         -2.1317e-02, -1.6838e-02,  3.1040e-03,  2.3777e-02,  3.9512e-02,\n",
            "         -1.0101e-02,  4.4093e-02,  6.2140e-02,  3.1284e-02, -7.2820e-02,\n",
            "         -3.3108e-02,  8.9993e-02,  1.0683e-01,  7.1537e-02, -9.8636e-02,\n",
            "         -8.8882e-02,  5.5291e-02,  1.8988e-02,  7.6844e-02,  6.5502e-02,\n",
            "         -4.2900e-02,  8.1508e-02,  2.1051e-02,  1.7779e-02,  4.7294e-02,\n",
            "          6.8580e-02, -1.3111e-03, -7.5054e-02,  2.2841e-02,  3.6271e-02,\n",
            "         -1.1831e-02, -5.8462e-02,  6.9337e-02, -9.1488e-02,  4.5565e-03,\n",
            "          7.7818e-02, -9.8243e-02, -4.1643e-02, -2.8591e-02,  3.5481e-02,\n",
            "         -2.2403e-02,  8.7178e-02,  3.9982e-02,  3.4038e-02,  2.7655e-02,\n",
            "          9.4823e-02, -8.5791e-02, -8.4794e-02, -8.4251e-02, -1.6249e-02,\n",
            "          5.7821e-02, -9.6093e-02,  1.0755e-01, -1.0217e-01, -1.7997e-02,\n",
            "          2.6430e-02,  1.0738e-01,  2.6504e-02,  2.9746e-03,  9.7594e-02,\n",
            "          6.0712e-02, -3.9936e-02, -1.6919e-02, -7.9801e-02, -3.3644e-02,\n",
            "          9.0572e-02, -7.7862e-02,  6.9729e-02,  8.8937e-02],\n",
            "        [-4.5956e-02,  5.3366e-02, -7.4739e-03, -2.6924e-03,  1.4466e-02,\n",
            "          5.4707e-02,  2.8526e-02, -4.4430e-02, -6.9394e-02,  1.0150e-01,\n",
            "         -9.1688e-02, -1.6365e-02, -7.5826e-02, -9.6404e-02, -5.3001e-02,\n",
            "         -3.4198e-02,  5.5582e-02,  6.2778e-03, -6.9825e-02,  8.8904e-02,\n",
            "          4.3875e-02, -7.7050e-02,  5.5357e-02,  3.5976e-02,  9.7316e-02,\n",
            "          1.5889e-02, -4.7673e-02,  1.0816e-01,  6.3940e-02, -5.5194e-02,\n",
            "          1.2985e-02,  1.5503e-02, -6.7654e-02,  6.3688e-02, -7.2006e-02,\n",
            "         -3.7349e-02,  1.0188e-01, -3.2144e-02, -7.6375e-02,  8.6377e-02,\n",
            "          8.5805e-02, -2.4601e-02, -8.1852e-02,  4.3179e-02, -3.3130e-02,\n",
            "          5.9926e-02, -6.1502e-02,  9.4053e-02, -1.0348e-01, -9.1126e-02,\n",
            "          3.3634e-02,  8.7028e-02,  2.1050e-02,  9.9370e-02, -6.8849e-02,\n",
            "         -6.7892e-02,  3.3038e-02,  9.7082e-02,  4.8680e-02, -8.7309e-02,\n",
            "         -1.0886e-01, -1.8241e-02, -7.0753e-02,  9.4883e-03, -4.0175e-02,\n",
            "          3.1713e-02, -1.2533e-02, -6.1162e-02,  8.5761e-02,  5.5212e-02,\n",
            "         -1.9972e-02,  3.8935e-02, -4.9372e-02, -5.8420e-02,  9.5799e-02,\n",
            "         -2.6003e-02,  8.4625e-02,  7.3850e-02,  7.3709e-02, -6.0677e-05,\n",
            "         -2.7197e-02, -7.0596e-02,  2.4973e-02, -7.6133e-03],\n",
            "        [ 3.8998e-02, -4.3050e-02, -6.9737e-02,  6.2217e-02,  1.0085e-02,\n",
            "          1.2523e-02,  5.3008e-03,  9.1438e-03, -2.9109e-02, -2.0598e-02,\n",
            "          4.3456e-02, -1.0431e-01, -7.8496e-02, -2.5896e-02, -2.4755e-02,\n",
            "         -2.5519e-02, -1.6484e-02,  2.4382e-02, -3.6067e-02,  3.4095e-02,\n",
            "          7.9093e-02,  9.4237e-02,  1.0146e-01,  9.3717e-02,  7.0405e-02,\n",
            "         -1.1667e-02, -3.6616e-02,  1.1421e-02,  2.8401e-02,  3.7738e-02,\n",
            "         -2.4373e-02, -2.5146e-03,  4.9180e-02, -7.6165e-02, -8.4485e-02,\n",
            "         -7.5360e-02, -3.7077e-03,  2.4043e-04, -3.5829e-03,  2.9921e-02,\n",
            "         -9.7623e-02,  1.0708e-01,  7.4245e-02,  2.9987e-02,  9.1604e-02,\n",
            "         -7.9043e-02, -5.7393e-02,  6.3377e-02, -6.5766e-02,  7.1239e-02,\n",
            "         -1.0540e-01,  3.9416e-02, -1.0772e-01, -3.2241e-02,  1.4097e-02,\n",
            "         -5.3726e-02, -4.5090e-02, -1.0772e-01,  3.1287e-02, -1.0616e-01,\n",
            "         -2.0297e-02,  1.0980e-02,  1.0400e-01,  6.5946e-02, -4.8054e-02,\n",
            "          7.3383e-03,  9.1473e-02,  8.0170e-02, -9.7904e-02,  3.7000e-02,\n",
            "         -4.6706e-02,  1.0852e-01,  8.7490e-02, -7.5231e-02,  9.9423e-02,\n",
            "          5.9965e-02,  1.0868e-01,  1.2159e-02, -8.3313e-02, -3.3652e-02,\n",
            "         -6.4394e-02,  4.5500e-02, -1.0393e-01, -8.5060e-02]],\n",
            "       requires_grad=True), Parameter containing:\n",
            "tensor([ 0.0416, -0.0097,  0.0361, -0.0149, -0.0573, -0.0307, -0.0598,  0.0559,\n",
            "         0.0930, -0.0913], requires_grad=True)]\n",
            "10\n",
            "torch.Size([6, 1, 3, 3])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlvuG_V5lGKE"
      },
      "source": [
        "Let's try a random 32x32 input.\n",
        "Note: expected input size of this net (LeNet) is 32x32. To use this net on\n",
        "the MNIST dataset, please resize the images from the dataset to 32x32.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QvO94mNhlGKF",
        "outputId": "20552103-9e19-45a0-8aa1-9619a08fc0f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "input = torch.randn(1, 1, 32, 32)\n",
        "out = net(input)\n",
        "print(out)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 0.0441, -0.0147,  0.0315, -0.0575, -0.0876, -0.0891, -0.0589,  0.0349,\n",
            "          0.0836, -0.0836]], grad_fn=<AddmmBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5W9-CHklGKH"
      },
      "source": [
        "Zero the gradient buffers of all parameters and backprops with random\n",
        "gradients:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29zP1lfFlGKI"
      },
      "source": [
        "net.zero_grad()\n",
        "out.backward(torch.randn(1, 10))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOdRaumHlGKK"
      },
      "source": [
        "<div class=\"alert alert-info\"><h4>Note</h4><p>``torch.nn`` only supports mini-batches. The entire ``torch.nn``\n",
        "    package only supports inputs that are a mini-batch of samples, and not\n",
        "    a single sample.\n",
        "\n",
        "    For example, ``nn.Conv2d`` will take in a 4D Tensor of\n",
        "    ``nSamples x nChannels x Height x Width``.\n",
        "\n",
        "    If you have a single sample, just use ``input.unsqueeze(0)`` to add\n",
        "    a fake batch dimension.</p></div>\n",
        "\n",
        "Before proceeding further, let's recap all the classes you’ve seen so far.\n",
        "\n",
        "**Recap:**\n",
        "  -  ``torch.Tensor`` - A *multi-dimensional array* with support for autograd\n",
        "     operations like ``backward()``. Also *holds the gradient* w.r.t. the\n",
        "     tensor.\n",
        "  -  ``nn.Module`` - Neural network module. *Convenient way of\n",
        "     encapsulating parameters*, with helpers for moving them to GPU,\n",
        "     exporting, loading, etc.\n",
        "  -  ``nn.Parameter`` - A kind of Tensor, that is *automatically\n",
        "     registered as a parameter when assigned as an attribute to a*\n",
        "     ``Module``.\n",
        "  -  ``autograd.Function`` - Implements *forward and backward definitions\n",
        "     of an autograd operation*. Every ``Tensor`` operation creates at\n",
        "     least a single ``Function`` node that connects to functions that\n",
        "     created a ``Tensor`` and *encodes its history*.\n",
        "\n",
        "**At this point, we covered:**\n",
        "  -  Defining a neural network\n",
        "  -  Processing inputs and calling backward\n",
        "\n",
        "**Still Left:**\n",
        "  -  Computing the loss\n",
        "  -  Updating the weights of the network\n",
        "\n",
        "Loss Function\n",
        "-------------\n",
        "A loss function takes the (output, target) pair of inputs, and computes a\n",
        "value that estimates how far away the output is from the target.\n",
        "\n",
        "There are several different\n",
        "`loss functions <https://pytorch.org/docs/nn.html#loss-functions>`_ under the\n",
        "nn package .\n",
        "A simple loss is: ``nn.MSELoss`` which computes the mean-squared error\n",
        "between the input and the target.\n",
        "\n",
        "For example:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rEPKSD-SlGKK",
        "outputId": "3ec211fb-b405-4402-9eb2-4b6b79953867",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "output = net(input)\n",
        "target = torch.randn(10)  # a dummy target, for example\n",
        "target = target.view(1, -1)  # make it the same shape as output\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "loss = criterion(output, target)\n",
        "print(output)\n",
        "print(target)\n",
        "print(loss)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 0.0441, -0.0147,  0.0315, -0.0575, -0.0876, -0.0891, -0.0589,  0.0349,\n",
            "          0.0836, -0.0836]], grad_fn=<AddmmBackward>)\n",
            "tensor([[-0.5120, -0.8922, -1.5607, -1.0136,  0.7030,  1.9886, -0.7932, -0.6681,\n",
            "          1.0636,  0.2864]])\n",
            "tensor(1.1601, grad_fn=<MseLossBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8JFD1KSMlGKN"
      },
      "source": [
        "Now, if you follow ``loss`` in the backward direction, using its\n",
        "``.grad_fn`` attribute, you will see a graph of computations that looks\n",
        "like this:\n",
        "\n",
        "::\n",
        "\n",
        "    input -> conv2d -> relu -> maxpool2d -> conv2d -> relu -> maxpool2d\n",
        "          -> view -> linear -> relu -> linear -> relu -> linear\n",
        "          -> MSELoss\n",
        "          -> loss\n",
        "\n",
        "So, when we call ``loss.backward()``, the whole graph is differentiated\n",
        "w.r.t. the loss, and all Tensors in the graph that has ``requires_grad=True``\n",
        "will have their ``.grad`` Tensor accumulated with the gradient.\n",
        "\n",
        "For illustration, let us follow a few steps backward:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1ghykn7lGKN",
        "outputId": "a3a2535a-4752-439e-de22-469a7d554d71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "print(loss.grad_fn)  # MSELoss\n",
        "print(loss.grad_fn.next_functions[0][0])  # Linear\n",
        "print(loss.grad_fn.next_functions[0][0].next_functions[0][0])  # ReLU"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<MseLossBackward object at 0x7f8700c76198>\n",
            "<AddmmBackward object at 0x7f8700c764a8>\n",
            "<AccumulateGrad object at 0x7f8700c76198>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bgp_Z2o8nEk7",
        "outputId": "bc28cdc4-e5c7-4e5e-fad5-32325c7f94b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(loss.grad_fn.metadata)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2gYz4Ia4lGKQ"
      },
      "source": [
        "Backprop\n",
        "--------\n",
        "To backpropagate the error all we have to do is to ``loss.backward()``.\n",
        "You need to clear the existing gradients though, else gradients will be\n",
        "accumulated to existing gradients.\n",
        "\n",
        "\n",
        "Now we shall call ``loss.backward()``, and have a look at conv1's bias\n",
        "gradients before and after the backward.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_cDLQNxlGKQ",
        "outputId": "59d5519c-4b8d-480c-ebdf-9a0a9f98c046",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "net.zero_grad()     # zeroes the gradient buffers of all parameters\n",
        "\n",
        "print('conv1.bias.grad before backward')\n",
        "print(net.conv1.bias.grad)\n",
        "\n",
        "loss.backward()\n",
        "\n",
        "print('conv1.bias.grad after backward')\n",
        "print(net.conv1.bias.grad)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "conv1.bias.grad before backward\n",
            "tensor([0., 0., 0., 0., 0., 0.])\n",
            "conv1.bias.grad after backward\n",
            "tensor([-0.0019,  0.0075,  0.0062, -0.0009,  0.0092, -0.0027])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OlM1s81DlGKS"
      },
      "source": [
        "Now, we have seen how to use loss functions.\n",
        "\n",
        "**Read Later:**\n",
        "\n",
        "  The neural network package contains various modules and loss functions\n",
        "  that form the building blocks of deep neural networks. A full list with\n",
        "  documentation is `here <https://pytorch.org/docs/nn>`_.\n",
        "\n",
        "**The only thing left to learn is:**\n",
        "\n",
        "  - Updating the weights of the network\n",
        "\n",
        "Update the weights\n",
        "------------------\n",
        "The simplest update rule used in practice is the Stochastic Gradient\n",
        "Descent (SGD):\n",
        "\n",
        "     ``weight = weight - learning_rate * gradient``\n",
        "\n",
        "We can implement this using simple Python code:\n",
        "\n",
        ".. code:: python\n",
        "\n",
        "    learning_rate = 0.01\n",
        "    for f in net.parameters():\n",
        "        f.data.sub_(f.grad.data * learning_rate)\n",
        "\n",
        "However, as you use neural networks, you want to use various different\n",
        "update rules such as SGD, Nesterov-SGD, Adam, RMSProp, etc.\n",
        "To enable this, we built a small package: ``torch.optim`` that\n",
        "implements all these methods. Using it is very simple:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BBs5YXNFlGKS"
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# create your optimizer\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
        "\n",
        "# in your training loop:\n",
        "optimizer.zero_grad()   # zero the gradient buffers\n",
        "output = net(input)\n",
        "loss = criterion(output, target)\n",
        "loss.backward()\n",
        "optimizer.step()    # Does the update"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8HBNmqQGlGKU"
      },
      "source": [
        ".. Note::\n",
        "\n",
        "      Observe how gradient buffers had to be manually set to zero using\n",
        "      ``optimizer.zero_grad()``. This is because gradients are accumulated\n",
        "      as explained in the `Backprop`_ section.\n",
        "\n"
      ]
    }
  ]
}